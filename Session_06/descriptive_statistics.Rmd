# Explorative Analyse

## Daten einlesen
```{r input, echo = TRUE}
df <- read.table('C:/LearningR/R-Course-2015/data/03-1_aeh(m).txt', header = T)
#character ineffizient -->factor effizienter!!
str(df)

head(df)
tail(df, n = 3)
```

## Häufigkeitsverteilungen
```{r table, echo = T}
head(df$FILLER)

attach(df)
str(FILLER)

t1 <- table(FILLER)
table(FILLER) / length(FILLER)

t2 <- prop.table(table(FILLER)) #relative proportions

cumsum(table(FILLER)) #cumulative sum (Sammelhäufigkeit)
```


## Kuchendiagramme

```{r piechart, echo = TRUE}
pie(table(FILLER))
```


## Balkendiagramme

```{r barplot, echo = T}
barplot(table(FILLER))
barplot(table(FILLER),
        main = 'Unser Balkendiagramm',
        # ylim
        # xlim
        col = c('red', 'green'), 
        names.arg = c('Aeh', 'Aehm', 'Volle Stille')
        )
#recycling -->Wiederholung eines Vectors wenn nicht ausreichend
c(2,4,6,8)/c(2,1)

```

## Klassenbildung

Sturges' Formel (Standardformel für Klassenbildung in hist())
$$
k = \log_2 n + 1
$$
Logarithmen als Umkehr von Potenzfuntionen
$$
2**x = n
$$

## Histogramme
```{r hist, echo = T} 
hist(LAENGE, breaks = c(200,350, 800, 1600),main = "Länge von Pausen in Millisekunden - theoretische Klasseneinteilung",freq = T)
hist(LAENGE, main = "Länge von Pausen in Millisekunden - Klasseneinteilung nach Sturges",freq = T)
hist(LAENGE, breaks = 4,main = "Länge von Pausen in Millisekunden",freq = T)

```

## Daten einlesen
```{r reading_XML, highlight = TRUE, results = 'hide', echo = TRUE, cache = TRUE}
library(XML)

tokens <- vector('character')
types <- vector('character')

xmlEventParse(
  "../data/t_990505_47.xml", 
  handlers = list(
    't' = function(name, attr) {
      tokens <<- c(tokens, attr['word'])
      types <<- c(types, attr['lemma'])
      ## morphology
      }
    ),
  addContext = FALSE
  )

#raw.token.lengths <- nchar(tokens)
# names(tokens) <- NULL
tokens <- unname(tokens)
token.lengths <- nchar(tokens)

```

```{r}
length(tokens)
```

## Wörter auswählen

```{r choose_words, echo = T}
tokens[nchar(tokens) == 22]
```

```{r}
#raw.token.lengths
```

## Modus
```{r mode, echo = T}
mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

#token.lengths
t1 <- table(token.lengths)
t1 
mode(token.lengths)
```

## Arbeit mit Tabellen
```{r tables, echo = T}
summary(t1)
t1[1]
dimnames(t1)
dim(t1)
```

## Median
```{r median, echo = T}
median(token.lengths)
length(token.lengths)
head(sort(token.lengths), length(token.lengths)/2)
t2 <- prop.table(table(token.lengths))
cumsum(t2)
```

```{r descriptive_values, echo = T}
mode(token.lengths)
median(token.lengths)
mean(token.lengths)

```
```{r}
hist(token.lengths)
```

```{r symmetric_data, echo = T}
set.seed(12)
normal.sample <- rnorm(100)
mode(normal.sample)
median(normal.sample)
mean(normal.sample)
```


## Quantile
```{r quantile, echo = T}
#quantile()
```

## Treppenfunktionen
```{r, functionplot, echo = T}

plot(cumsum(t2), type = 'S')

```

## Aufgaben

* Ein Dataframe mit Wortlängen und jeweiligen Wortformen aus einer XML-Datei erstellen.
* Eine gruppierte Datenreihe aus den rohen Daten konstruieren, die die Frequenzen berücksichtigt
  (welcher Datentyp ist das?).
* Diese Häufigkeitsverteilung mit einem Histogramm und einem Balkendiagramm darstellen.
* Ein Balkendiagramm mit der Bibliothek `ggplot2` mit denselben Daten erstellen (oder mindestens versuchen).
* Deskriptive Werte für die Types berechnen (wie für Tokens bereits gemacht)
* Werte tabellarisch mit Fall, Häufigkeit, relative Häufigkeit, Summenhäufigkeit, relative Summenhäufigkeit darstellen
